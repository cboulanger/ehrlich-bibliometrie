# The code that follows was almost completely generated by GPT-4 following my prompts.

import os
import re
import json
import pandas as pd
import numpy as np
import requests
import matplotlib.ticker as ticker
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

def extract_year(metadata):
    if 'published-print' in metadata:
        pub_info = metadata['published-print']
    elif 'issued' in metadata:
        pub_info = metadata['issued']
    else:
        return None

    if 'date-parts' in pub_info:
        year = pub_info['date-parts'][0][0]
    elif 'raw' in pub_info:
        year = pub_info['raw'][:4]
    else:
        return None

    return int(year)

def extract_author(metadata):
    author = metadata.get('author', [])
    return author[0].get('family','') if len(author) > 0 else ''

def get_metadata(doi):
    cache_file = f'cache/{doi}.json'
    if os.path.exists(cache_file):
        with open(cache_file, 'r') as f:
            metadata = json.load(f)
    else:
        url = f'https://api.crossref.org/works/{doi}'
        response = requests.get(url)
        try:
            metadata = response.json()['message']
        except json.JSONDecodeError:
            print(f"CrossRef error: could not load metadata for {url}")
            metadata = None
        os.makedirs(os.path.dirname(cache_file), exist_ok=True)
        with open(cache_file, 'w') as f:
            json.dump(metadata, f)
    return metadata

def truncate(s, x):
    return s[:x] + '...' if len(s) > x else s

def create_corpus(corpus_dir):
    articles = []
    for filename in tqdm(os.listdir(corpus_dir), desc="Analyzing article corpus"):
        doi = filename.replace('_', '/').strip('.txt')
        with open(os.path.join(corpus_dir, filename), 'r', encoding='utf-8') as f:
            text = f.read()
            metadata = get_metadata(doi)
            if metadata is not None:
                year = extract_year(metadata)
                articles.append({
                    'doi': doi,
                    'text': text,
                    'year': year,
                    'title': metadata['title'][0],
                    'author': extract_author(metadata)
                })

    return pd.DataFrame(articles).sort_values(by='year')

def plot_occurrences(articles_df, regex_list, first_year=None, last_year=None):
    # Find the occurrences of each regex in the text files
    occurrences = {}
    for regex in regex_list:
        regex_occurrences = []
        for text in articles_df['text']:
            regex_occurrences.append(len(re.findall(regex, text)))
        articles_df[regex] = regex_occurrences

    # Group the occurrences by year
    grouped_occurrences = articles_df.groupby('year').agg({regex: sum for regex in regex_list})

    # Find the article with the most occurrences of the first regex in each year
    first_regex = regex_list[0]
    top_articles = articles_df.loc[articles_df.groupby('year')[first_regex].idxmax()]
    top_articles = top_articles.loc[top_articles[first_regex] > 0]
    if first_year is not None:
        top_articles = top_articles.loc[top_articles['year'] >= first_year]
    if last_year is not None:
        top_articles = top_articles.loc[top_articles['year'] <= last_year]
    top_articles = top_articles.reset_index(drop=True)

    years = grouped_occurrences.index
    n_years = len(years)
    colors = ['tab:blue', 'tab:orange']
    bar_width = 0.8 / len(regex_list)

    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 6), gridspec_kw={'width_ratios': [3, 1]})

    for idx, (regex, color) in enumerate(zip(regex_list, colors)):
        ax1.bar(np.arange(n_years) + idx * bar_width, grouped_occurrences[regex], width=bar_width, label=regex, color=color)
        # add number on top of bar chart
        if idx == 0:
            for i, v in enumerate(grouped_occurrences[regex]):
                ax1.text(i - 0.25, v + 0.25, str(v), fontsize=9)

    ax1.set_xticks(np.arange(n_years))
    ax1.set_xticklabels(years)
    ax1.xaxis.set_major_locator(ticker.MultipleLocator(base=5))
    ax1.xaxis.set_minor_locator(ticker.MultipleLocator(base=1))
    ax1.grid(axis='x', which='minor', linestyle=':', alpha=0.5)

    ax1.set_xlabel('Year')
    ax1.set_ylabel('Occurrences of search terms')
    ax1.legend()

    # Create the box with the numbered list of top articles
    top_articles_list = [
        f'{i+1}. {row["author"]} ({row["year"]}) {truncate(row["title"], 30)} [{row[first_regex]}]'
        for i, row in top_articles.iterrows()
    ]
    ax2.axis('off')
    for i, text in enumerate(top_articles_list):
        ax2.text(0, 1 - i * 0.05, text, fontsize=10, verticalalignment='top')

    plt.tight_layout()
    return plt
